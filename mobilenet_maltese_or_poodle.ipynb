{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gskumlehn/maltese-or-poodle/blob/main/mobilenet_maltese_or_poodle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nr33oWQ_OwZ_"
      },
      "source": [
        "\n",
        "Updates database from github repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-zM04CJVzy1",
        "outputId": "4a983c8e-4ad4-4e16-de97-692f6cca9def"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'maltese-or-poodle'...\n",
            "remote: Enumerating objects: 993, done.\u001b[K\n",
            "remote: Counting objects: 100% (993/993), done.\u001b[K\n",
            "remote: Compressing objects: 100% (989/989), done.\u001b[K\n",
            "remote: Total 993 (delta 13), reused 970 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (993/993), 32.16 MiB | 36.07 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ],
      "source": [
        "!rm -rf maltese-or-poodle\n",
        "!git clone https://github.com/gskumlehn/maltese-or-poodle.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjAN0u3ePKE6"
      },
      "source": [
        "Creates two MobileNet models that share the same architecture and pre-trained weights but operate on different input layers:\n",
        "\n",
        "model: Processes raw/original input data.\n",
        "\n",
        "augmented_data_model: Processes augmented versions of the input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2OmmDFVOgdJ",
        "outputId": "6d259c21-b9cb-4fc1-9cf6-0fdb099a6674"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-145d36900d23>:4: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  model=MobileNet(weights='imagenet', include_top=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "\u001b[1m17225924/17225924\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-145d36900d23>:5: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  augmented_data_model=MobileNet(weights='imagenet', include_top=False)\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.applications import MobileNet\n",
        "\n",
        "model=MobileNet(weights='imagenet', include_top=False)\n",
        "augmented_data_model=MobileNet(weights='imagenet', include_top=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta4OlNGNPhC0"
      },
      "source": [
        "Global average pooling is applied to extract compact feature vectors from the outputs of both the original data model and the augmented data model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i-72-d2DOlnE"
      },
      "outputs": [],
      "source": [
        "from keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "x=model.output\n",
        "x=GlobalAveragePooling2D()(x)\n",
        "\n",
        "augmented_data_x=augmented_data_model.output\n",
        "augmented_data_x=GlobalAveragePooling2D()(augmented_data_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CfGOJEZPwCH"
      },
      "source": [
        "Creates two separate models for processing original and augmented data, with dense layers added for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CUIXMFtAOrGf"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "\n",
        "x=Dense(50, activation='relu')(x)\n",
        "preds=Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "augmented_data_x=Dense(50, activation='relu')(augmented_data_x)\n",
        "augmented_data_preds=Dense(1, activation='sigmoid')(augmented_data_x)\n",
        "\n",
        "model=Model(inputs=model.input, outputs=preds)\n",
        "augmented_data_model=Model(inputs=augmented_data_model.input, outputs=augmented_data_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z08XsEa0P0Eo"
      },
      "source": [
        "Counts layers that have already been trained and set only new ones for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w2YklC8iQAS4"
      },
      "outputs": [],
      "source": [
        "not_trainable_layer_count = len(model.layers) -1\n",
        "\n",
        "for layer in model.layers[:not_trainable_layer_count]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[not_trainable_layer_count:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "for layer in augmented_data_model.layers[:not_trainable_layer_count]:\n",
        "    layer.trainable=False\n",
        "for layer in augmented_data_model.layers[not_trainable_layer_count:]:\n",
        "    layer.trainable=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cNRRM_vQOCH"
      },
      "source": [
        "The dataset directory, target image size, and batch size are defined for preparing and processing image data in TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sIsc5hewQb5p"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "base_dir = \"/content/maltese-or-poodle/classes\"\n",
        "image_size = (224, 224)\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnPnTDBSQZkA"
      },
      "source": [
        "The code loads and splits image data from base_dir into training (80%) and validation (20%) datasets, resizing images and batching them. It also retrieves and prints the class names for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBl1d0o2TnWl",
        "outputId": "2015cc7e-ff0a-4515-989e-f9aa237c49b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 558 files belonging to 2 classes.\n",
            "Using 447 files for training.\n",
            "Found 558 files belonging to 2 classes.\n",
            "Using 111 files for validation.\n",
            "Classes: ['maltese', 'poodle']\n"
          ]
        }
      ],
      "source": [
        "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    base_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "test_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    base_dir,\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=image_size,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "class_names = train_dataset.class_names\n",
        "print(\"Classes:\", class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5WU1fSJQpJk"
      },
      "source": [
        "Sets up a pipeline for augmenting and normalizing images in the training dataset and normalizing the test dataset. This improves generalization during training and ensures consistent input scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QVaC122tT80P"
      },
      "outputs": [],
      "source": [
        "from keras.layers import RandomFlip, RandomRotation, RandomZoom, Rescaling\n",
        "from keras.models import Sequential\n",
        "\n",
        "data_augmentation = Sequential([\n",
        "    RandomFlip(\"horizontal\"),\n",
        "    RandomRotation(0.2),\n",
        "    RandomZoom(0.2),\n",
        "])\n",
        "\n",
        "normalization_layer = Rescaling(1./255)\n",
        "\n",
        "augmented_train_dataset = train_dataset.map(lambda x, y: (data_augmentation(normalization_layer(x)), y))\n",
        "augmented_test_dataset = test_dataset.map(lambda x, y: (normalization_layer(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGz8UPQhQ2dB"
      },
      "source": [
        " Optimizes data pipelines by enabling prefetching, allowing data loading and preprocessing to run concurrently with model training or evaluation, improving computational efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wp2x7YT1UBuc"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "augmented_train_dataset = augmented_train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
        "augmented_test_dataset = augmented_test_dataset.prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5KrCXXcQ_OD"
      },
      "source": [
        " Bompiles both the original and augmented data models with the Adam optimizer, binary cross-entropy loss, and accuracy as the evaluation metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DDhHjRaaUG0L"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001),loss='binary_crossentropy',metrics=['accuracy'])\n",
        "augmented_data_model.compile(optimizer=Adam(learning_rate=0.0001),loss='binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSKyb7MQRC19"
      },
      "source": [
        "Sets epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PraQ5uh6DShm"
      },
      "outputs": [],
      "source": [
        "epochs=70"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiR0oeTcRL5N"
      },
      "source": [
        " Trains the models using the training dataset for a specified number of epochs and evaluates it using the validation dataset after each epoch to monitor its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLx36wz4UR0C",
        "outputId": "93af24cf-f20e-4332-f6bd-555a58fd964a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.4952 - loss: 0.7867 - val_accuracy: 0.4685 - val_loss: 0.7985\n",
            "Epoch 2/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.4804 - loss: 0.7869 - val_accuracy: 0.4505 - val_loss: 0.7920\n",
            "Epoch 3/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.4868 - loss: 0.7791 - val_accuracy: 0.4505 - val_loss: 0.7858\n",
            "Epoch 4/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.4588 - loss: 0.7921 - val_accuracy: 0.4685 - val_loss: 0.7799\n",
            "Epoch 5/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5097 - loss: 0.7579 - val_accuracy: 0.4685 - val_loss: 0.7746\n",
            "Epoch 6/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.4974 - loss: 0.7506 - val_accuracy: 0.4775 - val_loss: 0.7697\n",
            "Epoch 7/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 2s/step - accuracy: 0.5039 - loss: 0.7481 - val_accuracy: 0.4685 - val_loss: 0.7650\n",
            "Epoch 8/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.4883 - loss: 0.7626 - val_accuracy: 0.4685 - val_loss: 0.7601\n",
            "Epoch 9/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.4976 - loss: 0.7338 - val_accuracy: 0.4505 - val_loss: 0.7562\n",
            "Epoch 10/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.4906 - loss: 0.7502 - val_accuracy: 0.4685 - val_loss: 0.7521\n",
            "Epoch 11/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.5016 - loss: 0.7489 - val_accuracy: 0.4685 - val_loss: 0.7483\n",
            "Epoch 12/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.4931 - loss: 0.7403 - val_accuracy: 0.4865 - val_loss: 0.7449\n",
            "Epoch 13/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.4752 - loss: 0.7502 - val_accuracy: 0.4865 - val_loss: 0.7414\n",
            "Epoch 14/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5224 - loss: 0.7388 - val_accuracy: 0.4955 - val_loss: 0.7385\n",
            "Epoch 15/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.4863 - loss: 0.7333 - val_accuracy: 0.5135 - val_loss: 0.7357\n",
            "Epoch 16/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.4751 - loss: 0.7451 - val_accuracy: 0.5135 - val_loss: 0.7331\n",
            "Epoch 17/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.5166 - loss: 0.7225 - val_accuracy: 0.5135 - val_loss: 0.7306\n",
            "Epoch 18/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.5192 - loss: 0.7321 - val_accuracy: 0.5045 - val_loss: 0.7283\n",
            "Epoch 19/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - accuracy: 0.5028 - loss: 0.7285 - val_accuracy: 0.5045 - val_loss: 0.7261\n",
            "Epoch 20/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5282 - loss: 0.7306 - val_accuracy: 0.4955 - val_loss: 0.7242\n",
            "Epoch 21/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.5419 - loss: 0.7115 - val_accuracy: 0.5135 - val_loss: 0.7225\n",
            "Epoch 22/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - accuracy: 0.5411 - loss: 0.7100 - val_accuracy: 0.5135 - val_loss: 0.7210\n",
            "Epoch 23/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.5032 - loss: 0.7263 - val_accuracy: 0.5045 - val_loss: 0.7191\n",
            "Epoch 24/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.5599 - loss: 0.6958 - val_accuracy: 0.5045 - val_loss: 0.7178\n",
            "Epoch 25/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.4856 - loss: 0.7224 - val_accuracy: 0.5045 - val_loss: 0.7165\n",
            "Epoch 26/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.5107 - loss: 0.7197 - val_accuracy: 0.5135 - val_loss: 0.7151\n",
            "Epoch 27/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - accuracy: 0.5020 - loss: 0.7124 - val_accuracy: 0.5225 - val_loss: 0.7141\n",
            "Epoch 28/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 3s/step - accuracy: 0.5106 - loss: 0.7246 - val_accuracy: 0.5135 - val_loss: 0.7131\n",
            "Epoch 29/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.5131 - loss: 0.7131 - val_accuracy: 0.5225 - val_loss: 0.7120\n",
            "Epoch 30/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.5071 - loss: 0.7059 - val_accuracy: 0.5315 - val_loss: 0.7111\n",
            "Epoch 31/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.5376 - loss: 0.7011 - val_accuracy: 0.5405 - val_loss: 0.7104\n",
            "Epoch 32/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5422 - loss: 0.6945 - val_accuracy: 0.5495 - val_loss: 0.7095\n",
            "Epoch 33/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5464 - loss: 0.6934 - val_accuracy: 0.5405 - val_loss: 0.7090\n",
            "Epoch 34/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - accuracy: 0.5228 - loss: 0.7035 - val_accuracy: 0.5225 - val_loss: 0.7082\n",
            "Epoch 35/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.5283 - loss: 0.6961 - val_accuracy: 0.5225 - val_loss: 0.7076\n",
            "Epoch 36/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5158 - loss: 0.6993 - val_accuracy: 0.5135 - val_loss: 0.7069\n",
            "Epoch 37/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - accuracy: 0.5109 - loss: 0.7071 - val_accuracy: 0.5135 - val_loss: 0.7065\n",
            "Epoch 38/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2s/step - accuracy: 0.5379 - loss: 0.7002 - val_accuracy: 0.5135 - val_loss: 0.7059\n",
            "Epoch 39/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - accuracy: 0.5133 - loss: 0.7046 - val_accuracy: 0.5135 - val_loss: 0.7054\n",
            "Epoch 40/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - accuracy: 0.4784 - loss: 0.7235 - val_accuracy: 0.5135 - val_loss: 0.7050\n",
            "Epoch 41/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.4897 - loss: 0.7182 - val_accuracy: 0.5135 - val_loss: 0.7045\n",
            "Epoch 42/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 2s/step - accuracy: 0.4923 - loss: 0.7189 - val_accuracy: 0.5225 - val_loss: 0.7041\n",
            "Epoch 43/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - accuracy: 0.5042 - loss: 0.7103 - val_accuracy: 0.5135 - val_loss: 0.7037\n",
            "Epoch 44/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - accuracy: 0.5219 - loss: 0.7045 - val_accuracy: 0.5315 - val_loss: 0.7034\n",
            "Epoch 45/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5137 - loss: 0.7113 - val_accuracy: 0.5315 - val_loss: 0.7031\n",
            "Epoch 46/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.4866 - loss: 0.7179 - val_accuracy: 0.5315 - val_loss: 0.7028\n",
            "Epoch 47/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - accuracy: 0.5224 - loss: 0.6946 - val_accuracy: 0.5315 - val_loss: 0.7025\n",
            "Epoch 48/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.5254 - loss: 0.7008 - val_accuracy: 0.5315 - val_loss: 0.7022\n",
            "Epoch 49/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.5148 - loss: 0.7086 - val_accuracy: 0.5405 - val_loss: 0.7019\n",
            "Epoch 50/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.5265 - loss: 0.6911 - val_accuracy: 0.5405 - val_loss: 0.7016\n",
            "Epoch 51/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.5270 - loss: 0.6950 - val_accuracy: 0.5495 - val_loss: 0.7014\n",
            "Epoch 52/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5127 - loss: 0.7081 - val_accuracy: 0.5495 - val_loss: 0.7011\n",
            "Epoch 53/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 2s/step - accuracy: 0.5209 - loss: 0.6920 - val_accuracy: 0.5495 - val_loss: 0.7009\n",
            "Epoch 54/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.5122 - loss: 0.7067 - val_accuracy: 0.5586 - val_loss: 0.7006\n",
            "Epoch 55/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - accuracy: 0.5231 - loss: 0.7013 - val_accuracy: 0.5586 - val_loss: 0.7004\n",
            "Epoch 56/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.5514 - loss: 0.6860 - val_accuracy: 0.5495 - val_loss: 0.7001\n",
            "Epoch 57/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.5336 - loss: 0.6961 - val_accuracy: 0.5495 - val_loss: 0.6999\n",
            "Epoch 58/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - accuracy: 0.5010 - loss: 0.7183 - val_accuracy: 0.5495 - val_loss: 0.6998\n",
            "Epoch 59/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.4787 - loss: 0.7161 - val_accuracy: 0.5495 - val_loss: 0.6995\n",
            "Epoch 60/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5110 - loss: 0.7110 - val_accuracy: 0.5495 - val_loss: 0.6994\n",
            "Epoch 61/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5417 - loss: 0.6981 - val_accuracy: 0.5495 - val_loss: 0.6991\n",
            "Epoch 62/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5204 - loss: 0.6954 - val_accuracy: 0.5495 - val_loss: 0.6990\n",
            "Epoch 63/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5428 - loss: 0.6881 - val_accuracy: 0.5495 - val_loss: 0.6987\n",
            "Epoch 64/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5557 - loss: 0.6831 - val_accuracy: 0.5495 - val_loss: 0.6985\n",
            "Epoch 65/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.5126 - loss: 0.7126 - val_accuracy: 0.5495 - val_loss: 0.6984\n",
            "Epoch 66/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.5298 - loss: 0.6968 - val_accuracy: 0.5495 - val_loss: 0.6982\n",
            "Epoch 67/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - accuracy: 0.5510 - loss: 0.6850 - val_accuracy: 0.5495 - val_loss: 0.6980\n",
            "Epoch 68/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 2s/step - accuracy: 0.5544 - loss: 0.6865 - val_accuracy: 0.5495 - val_loss: 0.6978\n",
            "Epoch 69/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.5604 - loss: 0.6937 - val_accuracy: 0.5495 - val_loss: 0.6976\n",
            "Epoch 70/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.5597 - loss: 0.6786 - val_accuracy: 0.5495 - val_loss: 0.6975\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=test_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBM7oxnpOldb",
        "outputId": "80a68393-90f1-4a42-fc5e-d869ad394fbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 3s/step - accuracy: 0.4462 - loss: 1.0000 - val_accuracy: 0.3514 - val_loss: 1.0155\n",
            "Epoch 2/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.4388 - loss: 0.9737 - val_accuracy: 0.3423 - val_loss: 1.0036\n",
            "Epoch 3/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.4450 - loss: 0.9419 - val_accuracy: 0.3423 - val_loss: 0.9925\n",
            "Epoch 4/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.4616 - loss: 0.9287 - val_accuracy: 0.3514 - val_loss: 0.9813\n",
            "Epoch 5/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.4386 - loss: 0.9322 - val_accuracy: 0.3514 - val_loss: 0.9709\n",
            "Epoch 6/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.4474 - loss: 0.9366 - val_accuracy: 0.3514 - val_loss: 0.9610\n",
            "Epoch 7/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 3s/step - accuracy: 0.4329 - loss: 0.9156 - val_accuracy: 0.3604 - val_loss: 0.9515\n",
            "Epoch 8/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.4599 - loss: 0.9102 - val_accuracy: 0.3604 - val_loss: 0.9426\n",
            "Epoch 9/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - accuracy: 0.4554 - loss: 0.8828 - val_accuracy: 0.3604 - val_loss: 0.9339\n",
            "Epoch 10/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.4500 - loss: 0.8971 - val_accuracy: 0.3694 - val_loss: 0.9256\n",
            "Epoch 11/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.4257 - loss: 0.9248 - val_accuracy: 0.3784 - val_loss: 0.9174\n",
            "Epoch 12/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.4123 - loss: 0.9086 - val_accuracy: 0.3694 - val_loss: 0.9096\n",
            "Epoch 13/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.4083 - loss: 0.8773 - val_accuracy: 0.3694 - val_loss: 0.9025\n",
            "Epoch 14/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.4436 - loss: 0.8728 - val_accuracy: 0.3694 - val_loss: 0.8956\n",
            "Epoch 15/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.4422 - loss: 0.8740 - val_accuracy: 0.3964 - val_loss: 0.8888\n",
            "Epoch 16/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.3902 - loss: 0.8675 - val_accuracy: 0.4054 - val_loss: 0.8825\n",
            "Epoch 17/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.4558 - loss: 0.8507 - val_accuracy: 0.3874 - val_loss: 0.8767\n",
            "Epoch 18/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 2s/step - accuracy: 0.4435 - loss: 0.8354 - val_accuracy: 0.3784 - val_loss: 0.8710\n",
            "Epoch 19/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 3s/step - accuracy: 0.4567 - loss: 0.8436 - val_accuracy: 0.3784 - val_loss: 0.8656\n",
            "Epoch 20/70\n",
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.4358 - loss: 0.8241 - val_accuracy: 0.3964 - val_loss: 0.8607\n",
            "Epoch 21/70\n",
            "\u001b[1m 4/14\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 2s/step - accuracy: 0.4655 - loss: 0.8469"
          ]
        }
      ],
      "source": [
        "augmented_data_history = augmented_data_model.fit(\n",
        "    augmented_train_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=augmented_test_dataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)\n",
        "\n",
        "aug_loss, aug_accuracy = augmented_data_model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('Test augmented loss:', aug_loss)\n",
        "print('Test augmented accuracy:', aug_accuracy)"
      ],
      "metadata": {
        "id": "AO23oftJcqcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2CmhDkxSNmi"
      },
      "outputs": [],
      "source": [
        "model.save(f'{class_names[0]}-or-{class_names[1]}-mobilenet.h5')\n",
        "from google.colab import files\n",
        "files.download(f'{class_names[0]}-or-{class_names[1]}-mobilenet.h5')\n",
        "\n",
        "augmented_data_model.save(f'augmented-data-{class_names[0]}-or-{class_names[1]}-mobilenet.h5')\n",
        "from google.colab import files\n",
        "files.download(f'augmented-data-{class_names[0]}-or-{class_names[1]}-mobilenet.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osJeeCZIRYM2"
      },
      "source": [
        "Loads test images, preprocesses them, and makes predictions using both the original and augmented data models. It then prints out the predicted class (\"poodle\" or \"maltese\") for each test image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfboJcavUzO-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "test_dir = f'{base_dir}/test'\n",
        "test_paths = os.listdir(test_dir)\n",
        "for path in test_paths:\n",
        "  test_image = image.load_img(f'{test_dir}/{path}', target_size = (224, 224))\n",
        "\n",
        "  test_image = image.img_to_array(test_image)\n",
        "  test_image = np.expand_dims(test_image, axis = 0)\n",
        "  test_image = test_image/255\n",
        "\n",
        "  result = model.predict(test_image)\n",
        "  augmented_data_result = augmented_data_model.predict(test_image)\n",
        "\n",
        "  if result[0][0] < 0.5:\n",
        "      prediction = class_names[0]\n",
        "  else:\n",
        "      prediction = class_names[1]\n",
        "\n",
        "  if augmented_data_result[0][0] < 0.5:\n",
        "      augmentedData_prediction = class_names[0]\n",
        "  else:\n",
        "      augmentedData_prediction = class_names[0]\n",
        "\n",
        "  print(f'Prediction: {path} is {prediction}')\n",
        "  print(f'Augmented data prediction: {path} is {augmentedData_prediction}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zfyjo_URweC"
      },
      "source": [
        "Generates a line graph comparing the training and validation accuracy of two models (one trained with original data and the other with augmented data) across multiple epochs, helping to visualize the performance differences between them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xLzd3MdnRc9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "val_loss = history.history['val_loss']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "augmented_data_val_loss = augmented_data_history.history['val_loss']\n",
        "augmented_data_val_accuracy = augmented_data_history.history['val_accuracy']\n",
        "\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "ax = fig.add_subplot(121)\n",
        "ax.plot(val_loss)\n",
        "ax.plot(augmented_data_val_loss)\n",
        "ax.set_title(\"Validation Loss\")\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.legend()\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.plot(val_accuracy)\n",
        "ax2.plot(augmented_data_val_accuracy)\n",
        "ax2.set_title(\"Validation Accuracy\")\n",
        "ax2.set_xlabel(\"Epochs\")\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfWfADzbmw-W"
      },
      "outputs": [],
      "source": [
        "plt.clf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9n08MQKsTxr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "val_loss = history.history['val_loss']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "augmented_data_val_loss = augmented_data_history.history['val_loss']\n",
        "augmented_data_val_accuracy = augmented_data_history.history['val_accuracy']\n",
        "\n",
        "log_fit_acc = np.polyfit(np.log(epochs), val_loss, 1)\n",
        "log_fit_aug_acc = np.polyfit(np.log(epochs), augmented_data_val_loss, 1)\n",
        "\n",
        "log_fit_val_acc = np.polyfit(np.log(epochs), val_accuracy, 1)\n",
        "log_fit_aug_val_acc = np.polyfit(np.log(epochs), augmented_data_val_accuracy, 1)\n",
        "\n",
        "# Generate fitted values\n",
        "log_line_acc = log_fit_acc[0] * np.log(epochs) + log_fit_acc[1]\n",
        "log_line_aug_acc = log_fit_aug_acc[0] * np.log(epochs) + log_fit_aug_acc[1]\n",
        "\n",
        "log_line_val_acc = log_fit_val_acc[0] * np.log(epochs) + log_fit_val_acc[1]\n",
        "log_line_aug_val_acc = log_fit_aug_val_acc[0] * np.log(epochs) + log_fit_aug_val_acc[1]\n",
        "\n",
        "fig = plt.figure(figsize=(16,4))\n",
        "\n",
        "ax = fig.add_subplot(121)\n",
        "\n",
        "ax.plot(epochs, log_line_acc, 'b-', label='Log Fit: Validation loss')\n",
        "ax.plot(epochs, log_line_aug_acc, 'r-', label='Log Fit: Augmented Validation Loss')\n",
        "ax.set_title(\"Validation Loss\")\n",
        "ax.set_xlabel(\"Epochs\")\n",
        "ax.legend()\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "\n",
        "ax2.plot(epochs, log_line_val_acc, 'b-.', label='Log Fit: Validation Accuracy')\n",
        "ax2.plot(epochs, log_line_aug_val_acc, 'r-.', label='Log Fit: Augmented Validation Accuracy')\n",
        "ax2.set_title(\"Validation Accuracy\")\n",
        "ax2.set_xlabel(\"Epochs\")\n",
        "ax.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvT4TbBrRPqdifIxHhybwp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}